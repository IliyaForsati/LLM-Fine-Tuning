{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb8822a",
   "metadata": {},
   "source": [
    "### 0. Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be1a04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439a4a8d",
   "metadata": {},
   "source": [
    "### 1. Testing Model *GPT2*\n",
    "\n",
    "Use model with ***pipeline*** as a high-level helper.  \n",
    "you can see the model detaile [here](https://huggingface.co/openai-community/gpt2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "534e5493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=50) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of AI is \\xa0to get better at it.\\nWhat is the current state of AI?\\nThe top thinkers in our field are all focused on AI. The next major AI research group will be called the AI Institute for Artificial Intelligence (AII). This is'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"openai-community/gpt2\")\n",
    "\n",
    "pipe(\"The future of AI is \", max_length=50, num_return_sequences=1, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a6199d",
   "metadata": {},
   "source": [
    "the answers are false. for example:\n",
    "'US is a country in Ê»al-Arabia, the region with' or 'The future of AI is \\xa0beyond the human brain. It's coming'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e24cd0",
   "metadata": {},
   "source": [
    "### 2. Fine-Tuning\n",
    "\n",
    "Here we do ***Fine-Tuning*** with datasets ***wikitext***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f7382f",
   "metadata": {},
   "source": [
    "- #### 2-1. Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97c11e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "dataset = dataset.select(range(200)) # for fast running\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf10bb2",
   "metadata": {},
   "source": [
    "- ##### 2-2. Load Model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcd4fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea0304",
   "metadata": {},
   "source": [
    "- #### 2-3. Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca4014c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_dataset(ds):\n",
    "    return tokenizer(ds[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_dataset, batched=True, remove_columns=\"text\")\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2631442",
   "metadata": {},
   "source": [
    "- #### 2-4. Set up the *Data-Collator*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "abb92846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm = False, #gpt2 isn't a masked language model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55443cb",
   "metadata": {},
   "source": [
    "- #### 2-5. Define *Traning-Arguments* & initialize the *Trainer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cbe82cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine-tuned-gpt2\",  # Directory to save the model\n",
    "    overwrite_output_dir=True,       # Overwrite on Directory (If it already exists)\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size per device\n",
    "    save_steps=500,                  # Save checkpoint every 500 steps\n",
    "    save_total_limit=2,              # Keep only the last 2 checkpoints\n",
    "    logging_dir=\"./logs\",            # Directory for logs\n",
    "    logging_steps=100,               # Log every 100 steps\n",
    "    eval_strategy  =\"steps\",         # Evaluate every `eval_steps`\n",
    "    eval_steps=500,                  # Evaluation frequency\n",
    "    learning_rate=5e-5,              # Learning rate\n",
    "    weight_decay=0.01,               # Weight decay\n",
    "    fp16=True,                       # Use mixed precision (if GPU supports it)\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,  # Use the same dataset for evaluation\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bfb000",
   "metadata": {},
   "source": [
    "- #### 2-6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65f36ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Afshin\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 07:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=3.5896175130208334, metrics={'train_runtime': 485.7532, 'train_samples_per_second': 1.235, 'train_steps_per_second': 0.154, 'total_flos': 39193804800000.0, 'train_loss': 3.5896175130208334, 'epoch': 3.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7a5879",
   "metadata": {},
   "source": [
    "- #### 2-7. Save *Fine-Tuned-gpt2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c17205b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-gpt2\\\\tokenizer_config.json',\n",
       " './fine-tuned-gpt2\\\\special_tokens_map.json',\n",
       " './fine-tuned-gpt2\\\\vocab.json',\n",
       " './fine-tuned-gpt2\\\\merges.txt',\n",
       " './fine-tuned-gpt2\\\\added_tokens.json',\n",
       " './fine-tuned-gpt2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./fine-tuned-gpt2\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca1f280",
   "metadata": {},
   "source": [
    "- #### 2-8. Testing new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0bde21f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=50) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of AI is \\n\\nAI is already a highly complex subject that is still under research and development. Many of the problems that we face today are within the scope of the present research and development efforts. This requires a broad assessment of the present state of the art and'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"./fine-tuned-gpt2\")\n",
    "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(\"./fine-tuned-gpt2\")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=fine_tuned_tokenizer)\n",
    "\n",
    "pipe(\"The future of AI is \", max_length=50, num_return_sequences=1, max_new_tokens=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
